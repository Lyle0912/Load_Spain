{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp \n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import optuna\n",
    "import json\n",
    "import time\n",
    "import tensorflow.compat.v2.keras as keras\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"Final_dataset.csv\")\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data.set_index('DateTime', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1825*24\n",
    "data = data.iloc[:N] # 4 years of training\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = 'JSU'\n",
    "paramcount = {'Normal': 2,\n",
    "              'StudentT': 3,\n",
    "              'JSU': 4,\n",
    "              'Point': None\n",
    "              }\n",
    "val_multi = 4  # number of times the model will be retrained \n",
    "val_window = 364 // val_multi #28: size of each validation window, each validation window will cover 28 days.\n",
    "INP_SIZE = 123\n",
    "S= 24\n",
    "activations = ['sigmoid', 'relu', 'elu', 'tanh', 'softplus', 'softmax']\n",
    "\n",
    "binopt = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # prepare the input/output dataframes\n",
    "    Y = np.zeros((1825, S))\n",
    "    Yf = np.zeros((365, S))\n",
    "    for d in range(1825):\n",
    "        Y[d, :] = data.loc[data.index[d*24:(d+1)*24], 'Load_DA'].to_numpy()\n",
    "    # Y = Y[7:, :] # skip first 7 days\n",
    "    for d in range(365):\n",
    "        Yf[d, :] = data.loc[data.index[(\n",
    "            d+1460)*S:(d+1461)*S], 'Load_DA'].to_numpy()\n",
    "    #\n",
    "    X = np.zeros((1460+365, INP_SIZE))\n",
    "    for d in range(7, 1460+365):\n",
    "        \n",
    "        X[d, :24] = data.loc[data.index[(d-1)*S:d*S], 'Load_DA'].to_numpy()  # D-1 load\n",
    "        X[d, 24:48] = data.loc[data.index[(d-2)*S:(d-1)*S], 'Load_DA'].to_numpy()  # D-2 load\n",
    "        X[d, 48:72] = data.loc[data.index[(d-7)*S:(d-6)*S], 'Load_DA'].to_numpy()  # D-7 load\n",
    "        \n",
    "        X[d, 72:96] = data.loc[data.index[d*S:(d+1)*S], 'is_flex_holiday'].to_numpy()  # is_flex_holiday\n",
    "        X[d, 96:120] = data.loc[data.index[d*S:(d+1)*S], 'is_fixed_holiday'].to_numpy()  # is_fixed_holiday\n",
    "        X[d, 120] = data.loc[data.index[d*S:(d+1)*S:S], 'is_regional_holiday'].to_numpy()  # is_regional_holiday\n",
    "        X[d, 121] = data.loc[data.index[d*S:(d+1)*S:S], 'is_xmas'].to_numpy()  # is_xmas\n",
    "        X[d, 122] = data.index[d * 24].weekday() \n",
    "    \n",
    "    # '''\n",
    "    Xwhole = X.copy()\n",
    "    Ywhole = Y.copy()\n",
    "    Yfwhole = Yf.copy()\n",
    "    metrics_sub = []\n",
    "    \n",
    "    \n",
    "    for train_no in range(val_multi):\n",
    "        start = val_window * train_no\n",
    "        X = Xwhole[start:1460+start, :]\n",
    "        Xf = Xwhole[1460+start:1460+start+val_window, :]\n",
    "        Y = Ywhole[start:1460+start, :]\n",
    "        Yf = Ywhole[1460+start:1460+start+val_window, :]\n",
    "        X = X[7:1460, :]\n",
    "        Y = Y[7:1460, :]\n",
    "        # begin building a model\n",
    "        # <= INP_SIZE as some columns might have been turned off\n",
    "        inputs = keras.Input(X.shape[1])\n",
    "        # batch normalization- normalize the inputs\n",
    "       \n",
    "        batchnorm = True\n",
    "        if batchnorm:\n",
    "            norm = keras.layers.BatchNormalization()(inputs)\n",
    "            last_layer = norm\n",
    "        else:\n",
    "            last_layer = inputs\n",
    "        # dropout\n",
    "        dropout = trial.suggest_categorical('dropout', binopt)\n",
    "        if dropout:\n",
    "            rate = trial.suggest_float('dropout_rate', 0, 1)\n",
    "            drop = keras.layers.Dropout(rate)(last_layer)\n",
    "            last_layer = drop\n",
    "            \n",
    "            \n",
    "        # regularization of 1st hidden layer,\n",
    "        # activation - output, kernel - weights/parameters of input\n",
    "        regularize_h1_activation = trial.suggest_categorical(\n",
    "            'regularize_h1_activation', binopt)\n",
    "        regularize_h1_kernel = trial.suggest_categorical(\n",
    "            'regularize_h1_kernel', binopt)\n",
    "        h1_activation_rate = (0.0 if not regularize_h1_activation\n",
    "                              else trial.suggest_float('h1_activation_rate_l1', 1e-5, 1e1, log=True))\n",
    "        h1_kernel_rate = (0.0 if not regularize_h1_kernel\n",
    "                          else trial.suggest_float('h1_kernel_rate_l1', 1e-5, 1e1, log=True))\n",
    "        \n",
    "        # define 1st hidden layer with regularization\n",
    "        hidden = keras.layers.Dense(trial.suggest_int('neurons_1', 16, 128, log=False),\n",
    "                                    activation=trial.suggest_categorical(\n",
    "                                        'activation_1', activations),\n",
    "                                    # kernel_initializer='ones',\n",
    "                                    kernel_regularizer=keras.regularizers.L1(\n",
    "                                        h1_kernel_rate),\n",
    "                                    activity_regularizer=keras.regularizers.L1(h1_activation_rate))(last_layer)\n",
    "        \n",
    "        \n",
    "        # regularization of 2nd hidden layer,\n",
    "        # activation - output, kernel - weights/parameters of input\n",
    "        regularize_h2_activation = trial.suggest_categorical(\n",
    "            'regularize_h2_activation', binopt)\n",
    "        regularize_h2_kernel = trial.suggest_categorical(\n",
    "            'regularize_h2_kernel', binopt)\n",
    "        h2_activation_rate = (0.0 if not regularize_h2_activation\n",
    "                              else trial.suggest_float('h2_activation_rate_l1', 1e-5, 1e1, log=True))\n",
    "        h2_kernel_rate = (0.0 if not regularize_h2_kernel\n",
    "                          else trial.suggest_float('h2_kernel_rate_l1', 1e-5, 1e1, log=True))\n",
    "        # define 2nd hidden layer with regularization\n",
    "        hidden = keras.layers.Dense(trial.suggest_int('neurons_2', 16, 128, log=False),\n",
    "                                    activation=trial.suggest_categorical(\n",
    "                                        'activation_2', activations),\n",
    "                                    # kernel_initializer='ones',\n",
    "                                    kernel_regularizer=keras.regularizers.L1(\n",
    "                                        h2_kernel_rate),\n",
    "                                    activity_regularizer=keras.regularizers.L1(h2_activation_rate))(hidden)\n",
    "        \n",
    "        ### DNN\n",
    "        if paramcount[distribution] is None:\n",
    "            outputs = keras.layers.Dense(24, activation='linear')(hidden)\n",
    "            model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=keras.optimizers.Adam(trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)),\n",
    "                          loss='mae',\n",
    "                          metrics='mae')\n",
    "        else:\n",
    "            # define parameter layers with their regularization\n",
    "            param_layers = []\n",
    "            param_names = [\"loc\", \"scale\", \"tailweight\", \"skewness\", \"power\"]\n",
    "            for p in range(paramcount[distribution]):\n",
    "                regularize_param_kernel = trial.suggest_categorical(\n",
    "                    'regularize_'+param_names[p], binopt)\n",
    "                param_kernel_rate = (0.0 if not regularize_param_kernel\n",
    "                                     else trial.suggest_float(param_names[p]+'_rate_l1', 1e-5, 1e1, log=True))\n",
    "                param_layers.append(keras.layers.Dense(\n",
    "                    24, activation='linear',  # kernel_initializer='ones',\n",
    "                    kernel_regularizer=keras.regularizers.L1(param_kernel_rate))(hidden))\n",
    "        \n",
    "        \n",
    "        # Modeling\n",
    "            linear = tf.keras.layers.concatenate(param_layers)\n",
    "            # define outputs\n",
    "            if distribution == 'Normal':\n",
    "                outputs = tfp.layers.DistributionLambda(\n",
    "                    lambda t: tfd.Normal(\n",
    "                        loc=t[..., :24],\n",
    "                        scale=1e-3 + 3 * tf.math.softplus(t[..., 24:])))(linear)\n",
    "            elif distribution == 'JSU':\n",
    "                outputs = tfp.layers.DistributionLambda(\n",
    "                    lambda t: tfd.JohnsonSU(\n",
    "                        loc=t[..., :24],\n",
    "                        scale=1e-3 + 3 * tf.math.softplus(t[..., 24:48]),\n",
    "                        tailweight=1 + 3 * tf.math.softplus(t[..., 48:72]),\n",
    "                        skewness=t[..., 72:]))(linear)\n",
    "            elif distribution == 'NormalInverseGaussian':\n",
    "                outputs = tfp.layers.DistributionLambda(\n",
    "                    lambda t: tfd.NormalInverseGaussian(\n",
    "                        loc=t[..., :24],\n",
    "                        scale=1e-3 + 3 * tf.math.softplus(t[..., 24:48]),\n",
    "                        tailweight=1e-3 + 3 * tf.math.softplus(t[..., 48:72]),\n",
    "                        skewness=t[..., 72:]))(linear)\n",
    "            elif distribution == 'GeneralizedNormal':  \n",
    "                outputs = tfp.layers.DistributionLambda(\n",
    "                    lambda t: tfd.GeneralizedNormal(\n",
    "                        loc=t[..., :24],\n",
    "                        scale=1e-3 + 3 * tf.math.softplus(t[..., 24:48]),\n",
    "                        power=1e-3 + 3 * tf.math.softplus(t[..., 48:])))(linear)\n",
    "    \n",
    "            elif distribution == 'StudentT':\n",
    "                outputs = tfp.layers.DistributionLambda(\n",
    "                    lambda t: tfd.StudentT(\n",
    "                        loc=t[..., :24],\n",
    "                        scale=1e-3 + 3 * tf.math.softplus(t[..., 24:48]),\n",
    "                        df=1 + 3 * tf.math.softplus(t[..., 48:])))(linear)    \n",
    "            else:\n",
    "                raise ValueError(f'Incorrect distribution {distribution}')\n",
    "            model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=keras.optimizers.Adam(trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)),\n",
    "                          loss=lambda y, rv_y: -rv_y.log_prob(y), #minimize the negative log-likelihood\n",
    "                          metrics='mae')\n",
    "        # '''\n",
    "        \n",
    "        # define callbacks\n",
    "        callbacks = [keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True)]\n",
    "        model.fit(X, Y, epochs=100, validation_data=(Xf, Yf),callbacks=callbacks, batch_size=32, verbose=True)\n",
    "\n",
    "        # for point its a list of one [loss, MAE]\n",
    "        metrics = model.evaluate(Xf, Yf)\n",
    "        metrics_sub.append(metrics[0])\n",
    "        avg_metric = np.mean(metrics_sub)\n",
    "        # Update the best parameters if this trial has better performance\n",
    "        if 'best_metric' not in trial.user_attrs or avg_metric < trial.user_attrs['best_metric']:\n",
    "            trial.user_attrs['best_metric'] = avg_metric\n",
    "            trial.user_attrs['best_params'] = trial.params\n",
    "        # we optimize the returned value, -1 will always take the model with best MAE\n",
    "    return avg_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Optuna study\n",
    "study_JSU = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Optimize the study\n",
    "study_JSU.optimize(objective, n_trials=50)\n",
    "\n",
    "# Access the best hyperparameters\n",
    "best_params_JSU = study_JSU.best_params\n",
    "print(\"Best hyperparameters:\", best_params_JSU)\n",
    "\n",
    "# # Save the best set of parameters\n",
    "# np.save(\"best_params.npy\", study.best_trial.user_attrs['best_params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
