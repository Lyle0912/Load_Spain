{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "import tensorflow_probability as tfp \n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow.compat.v2.keras as keras\n",
    "from datetime import datetime\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm, johnsonsu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"Final_dataset.csv\")\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data = data.drop(columns= ['is_holiday', 'temp_actual','wspd_actual', 'temp_24hour_MA', 'temp_48hour_MA', 'temp_72hour_MA','Name',])\n",
    "data.set_index('DateTime', inplace=True)\n",
    "#data = data.loc['2023-12-01 09:00:00':] # 2023-04-01 to 2024-03-31\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_forecast = pd.read_csv(\"weatherforecast.csv\")\n",
    "# weather_forecast = weather_forecast[(weather_forecast['horizon'] >= 1) & (weather_forecast['horizon'] <= 168)]\n",
    "# weather_forecast = weather_forecast.drop(columns= ['forecast', 'wspd_forecast'])\n",
    "# weather_forecast['DateTime'] = pd.to_datetime(weather_forecast['DateTime'])\n",
    "# weather_forecast.set_index('DateTime', inplace=True)\n",
    "# weather_forecast.index = weather_forecast.index.tz_convert(None)\n",
    "# weather_forecast.reset_index(inplace=True)\n",
    "# data.reset_index(inplace=True)\n",
    "# data_expanded = data.loc[data.index.repeat(168)].reset_index(drop=True)\n",
    "# data_expanded['horizon'] = list(range(1, 169)) * len(data)\n",
    "# combined_data = pd.merge(data_expanded, weather_forecast, on=['DateTime', 'horizon'], how='left')\n",
    "# combined_data.set_index('DateTime', inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.set_index('DateTime', inplace=True)\n",
    "#data.index = pd.to_datetime(data.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = 'JSU' # distribution for forecast\n",
    "paramcount = {'Normal': 2,\n",
    "              'JSU': 4\n",
    "              }\n",
    "INP_SIZE =  123\n",
    "S=24\n",
    "activations = ['sigmoid', 'relu', 'elu', 'tanh', 'softplus', 'softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runoneday(inp):\n",
    "    params, dayno = inp\n",
    "    \n",
    "    # Define the start date for the forecast based on dayno\n",
    "    start_date = pd.Timestamp('2023-12-01 09:00:00') + pd.Timedelta(days=dayno)\n",
    "    \n",
    "   \n",
    "    df = data.loc[start_date:] \n",
    "    \n",
    "\n",
    "    # Compute the size of the dataset\n",
    "    num_days = len(df) // 24\n",
    "    \n",
    "\n",
    "    \n",
    "    # Initialize Y matrix for target values\n",
    "    Y = np.zeros((num_days - 7, 168))  \n",
    "    for d in range(num_days - 7):\n",
    "        start_idx = d * 24\n",
    "        end_idx = (d + 7) * 24\n",
    "        Y[d, :] = df.loc[df.index[start_idx:end_idx], 'Load_DA'].to_numpy()\n",
    "        \n",
    "\n",
    "    # Initialize X matrix for input features\n",
    "    X = np.zeros((num_days, INP_SIZE))  # Adjust X size for the full dataset\n",
    "    for d in range(7, num_days):\n",
    "        \n",
    "        X[d, :24] = df.loc[df.index[(d-1)*S:d*S], 'Load_DA'].to_numpy()  # D-1 load\n",
    "        X[d, 24:48] = df.loc[df.index[(d-2)*S:(d-1)*S], 'Load_DA'].to_numpy()  # D-2 load\n",
    "        X[d, 48:72] = df.loc[df.index[(d-7)*S:(d-6)*S], 'Load_DA'].to_numpy()  # D-7 load\n",
    "        \n",
    "        X[d, 72:96] = df.loc[df.index[d*S:(d+1)*S], 'is_flex_holiday'].to_numpy()  # is_flex_holiday\n",
    "        X[d, 96:120] = df.loc[df.index[d*S:(d+1)*S], 'is_fixed_holiday'].to_numpy()  # is_fixed_holiday\n",
    "        X[d, 120] = df.loc[df.index[d*S:(d+1)*S:S], 'is_regional_holiday'].to_numpy()  # is_regional_holiday\n",
    "        X[d, 121] = df.loc[df.index[d*S:(d+1)*S:S], 'is_xmas'].to_numpy()  # is_xmas\n",
    "        X[d, 122] = df.index[d * 24].weekday()  # weekday \n",
    "        \n",
    "\n",
    "    \n",
    "    Xf = X[-1:, :] \n",
    "    X = X[7:, :]  # Excluding the first 7 rows for training\n",
    "\n",
    "    # Model Building\n",
    "    inputs = keras.Input(shape=(X.shape[1],))\n",
    "    last_layer = inputs\n",
    "    if params.get('batchnorm', False):\n",
    "        last_layer = keras.layers.BatchNormalization()(inputs)\n",
    "    if params.get('dropout', False):\n",
    "        last_layer = keras.layers.Dropout(params['dropout_rate'])(last_layer)\n",
    "\n",
    "    # First hidden layer with regularization\n",
    "    hidden = keras.layers.Dense(params['neurons_1'],\n",
    "                                activation=params['activation_1'],\n",
    "                                kernel_regularizer=keras.regularizers.L1(\n",
    "                                    params.get('h1_kernel_rate_l1', 0.0)),\n",
    "                                activity_regularizer=keras.regularizers.L1(\n",
    "                                    params.get('h1_activation_rate_l1', 0.0)))(last_layer)\n",
    "\n",
    "    # Second hidden layer with regularization\n",
    "    hidden = keras.layers.Dense(params['neurons_2'],\n",
    "                                activation=params['activation_2'],\n",
    "                                kernel_regularizer=keras.regularizers.L1(\n",
    "                                    params.get('h2_kernel_rate_l1', 0.0)),\n",
    "                                activity_regularizer=keras.regularizers.L1(\n",
    "                                    params.get('h2_activation_rate_l1', 0.0)))(hidden)\n",
    "\n",
    "    param_layers = []\n",
    "    param_names = [\"loc\", \"scale\", \"tailweight\", \"skewness\", \"power\"]  # distribution type\n",
    "\n",
    "    for p in range(paramcount[distribution]):\n",
    "        regularize_param_kernel = params['regularize_' + param_names[p]]\n",
    "        param_kernel_rate = (0.0 if not regularize_param_kernel\n",
    "                             else params[str(param_names[p]) + '_rate_l1'])\n",
    "        param_layers.append(keras.layers.Dense(\n",
    "            168, activation='linear',  # kernel_initializer='ones',\n",
    "            kernel_regularizer=keras.regularizers.L1(param_kernel_rate))(hidden))\n",
    "\n",
    "    linear = tf.keras.layers.concatenate(param_layers)\n",
    "\n",
    "    if distribution == 'Normal':\n",
    "        outputs = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Normal(loc=t[..., :168], scale=1e-3 + 3 * tf.math.softplus(t[..., 168:])))(linear)\n",
    "    elif distribution == 'JSU':\n",
    "        outputs = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.JohnsonSU(\n",
    "                loc=t[..., :168],\n",
    "                scale=1e-3 + 3 * tf.math.softplus(t[..., 168:336]),\n",
    "                tailweight=1 + 3 * tf.math.softplus(t[..., 336:504]),\n",
    "                skewness=t[..., 504:]))(linear)\n",
    "    else:\n",
    "        raise ValueError(f'Incorrect distribution {distribution}')\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(params['learning_rate']),\n",
    "                  loss=lambda y, rv_y: -rv_y.log_prob(y),\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    callbacks = [keras.callbacks.EarlyStopping(patience=50, restore_best_weights=True)]\n",
    "\n",
    "    perm = np.random.permutation(np.arange(X.shape[0]))\n",
    "    VAL_DATA = 0.2\n",
    "    trainsubset = perm[:int((1 - VAL_DATA) * len(perm))]\n",
    "    valsubset = perm[int((1 - VAL_DATA) * len(perm)):]\n",
    "\n",
    "    model.fit(X[trainsubset], Y[trainsubset], epochs=100, validation_data=(X[valsubset], Y[valsubset]),\n",
    "              callbacks=callbacks, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "    Y_real = Y[dayno:dayno+1, :]\n",
    "    x_row = X[dayno:dayno+1, :]\n",
    "    \n",
    "    print(f\"x_row shape: {x_row.shape}\")\n",
    "    dist = model(x_row)\n",
    "    Y_pred = model.predict(x_row)\n",
    "    \n",
    "    columns = {\n",
    "        'real': Y_real.flatten(),\n",
    "        'predicted': Y_pred.flatten(),\n",
    "        'loc': dist.loc.numpy().flatten(), \n",
    "        'scale': dist.scale.numpy().flatten(), \n",
    "    }\n",
    "    if distribution == 'Normal':\n",
    "       pass\n",
    "    elif distribution == 'JSU':\n",
    "        columns['tailweight'] = dist.tailweight.numpy().flatten()\n",
    "        columns['skewness'] = dist.skewness.numpy().flatten()\n",
    "\n",
    "    # Add forecast parameters\n",
    "    # y_len = Y.shape[0]\n",
    "    # df_idx_start = len(df) - (y_len - dayno) * 24\n",
    "    df_forecast = pd.DataFrame(columns, index=df.index[0:168])\n",
    "    # Save the DataFrame to CSV with correct date range\n",
    "    filename = f'forecast_with_parameters_{dayno}.csv'\n",
    "    df_forecast.to_csv(filename)\n",
    "    print('saved to: ', filename)\n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point forecasting\n",
    "best_params ={'dropout': True,\n",
    " 'dropout_rate': 0.005579447300070095,\n",
    " 'regularize_h1_activation': True,\n",
    " 'regularize_h1_kernel': False,\n",
    " 'h1_activation_rate_l1': 9.510278864916939e-05,\n",
    " 'neurons_1': 75,\n",
    " 'activation_1': 'elu',\n",
    " 'regularize_h2_activation': False,\n",
    " 'regularize_h2_kernel': True,\n",
    " 'h2_kernel_rate_l1': 0.002614976865913897,\n",
    " 'neurons_2': 55,\n",
    " 'activation_2': 'relu',\n",
    " 'regularize_loc': False,\n",
    " 'regularize_scale': False,\n",
    " 'regularize_tailweight': True,\n",
    " 'tailweight_rate_l1': 0.0017295629298938247,\n",
    " 'regularize_skewness': False,\n",
    " 'learning_rate': 0.002625275649801325}\n",
    "inputlist = [(best_params, day) for day in range(len(data) // 24)]\n",
    "print(len(inputlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params ={\n",
    " 'dropout': False,\n",
    " 'regularize_h1_activation': True,\n",
    " 'regularize_h1_kernel': False,\n",
    " 'h1_activation_rate_l1': 1.3411937832055677e-05,\n",
    " 'neurons_1': 71,\n",
    " 'activation_1': 'relu',\n",
    " 'regularize_h2_activation': False,\n",
    " 'regularize_h2_kernel': True,\n",
    " 'h2_kernel_rate_l1': 0.0004665633669008087,\n",
    " 'neurons_2': 53,\n",
    " 'activation_2': 'softplus',\n",
    " 'regularize_loc': False,\n",
    " 'regularize_scale': False,\n",
    " 'learning_rate': 0.09157521767426445}\n",
    "inputlist = [(best_params, day) for day in range(len(data) // 24)]\n",
    "print(len(inputlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_forecasts_NN = pd.DataFrame()\n",
    "\n",
    "\n",
    "for e in inputlist:\n",
    "    forecast= runoneday(e)\n",
    "    # all_forecasts_NN = pd.concat([all_forecasts_NN, forecast])\n",
    "# # Save all forecasts to a single CSV file\n",
    "# all_forecasts_NN.to_csv('forecasts_NN_jsu2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
